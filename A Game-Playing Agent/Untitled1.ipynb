{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Review\n",
    "The field of Artificial lIntelligence is continually changing and advancing. To be an AI Engineer at the cutting edge of your field, you'll need to be able to read and communicate some of these advancements with your peers. In order to help you get comfortable with this, in the second part of this project you will read a seminal paper in the field of Game-Playing and write a simple one page summary on it. Here are your instructions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a Game-Playing paper from the following list or another of your choosing:\n",
    "\n",
    "- [Game Tree Searching by Min / Max Approximation](https://people.csail.mit.edu/rivest/pubs/Riv87c.pdf) by Ron Rivest, MIT (Fun fact, Ron Rivest is the R is in the RSA cryptographic protocol).\n",
    "- [Deep Blue](https://pdfs.semanticscholar.org/ad2c/1efffcd7c3b7106e507396bdaa5fe00fa597.pdf) by the IBM Watson Team (Fun fact, Deep Blue beat Gary Kasparov in Chess in one of the most famous AI spectacles of the 20th century).\n",
    "- [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) by the DeepMind Team.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a simple one page summary of the paper covering the following:\n",
    "\n",
    "* A brief summary of the paper's goals or techniques introduced (if any).\n",
    "* A brief summary of the paper's results (if any).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title:       ** Mastering the game of Go with deep neural networks and tree search **\n",
    "\n",
    "\n",
    "### Personal Reason to choose AlphaGod: \n",
    "**I have chose AlphaGo by DeepMind Team because it is about game of Go and a computer program never defeated a huma professional player in a full size game of Go.**\n",
    "\n",
    "\n",
    "### Introduction: \n",
    "**The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Deep Mind team introduced a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game of Go Difficulties: \n",
    "**infeasible exhaustive search**: In large games, such as chess (b≈35, d≈80) and especially Go (b≈250, d≈150), exhaustive search is infeasible.\n",
    "\n",
    "---\n",
    "### Earlier approaches: \n",
    "1. **First**, the depth of the search may be reduced by position evaluation: truncating the search tree at state s and replacing the subtree below s by an approximate value function that predicts the outcomefrom state s. \n",
    "2. **Second**, the breadth of the search may be reduced by sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in position s.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepMind Approach: \n",
    "\n",
    "**DeepMind introduce a new approach** to computer Go that uses **‘value networks’** to evaluate board positions and **‘policy networks’** to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and **reinforcement learning** from games of self-play. \n",
    "\n",
    "Deep Mind also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods used by Deep Mind \n",
    "\n",
    "<img src=\"nature16961-f3.jpg\" width=\"60%\", style=\"float: right\">\n",
    "1. **Monte Carlo tree search** : estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values.\n",
    "\n",
    "2. **policy network**: Used Convolutional Neural Network to read the board as an image and finds the positions on the board, then using supervised learning from human moves to learn the game following by Reinforcement learning to improve the network.\n",
    "\n",
    "3. **value network**: evaluate board position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "1. **  By combining tree search with policy and value networks, AlphaGo has finally reached a professional level in Go, providing hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains **\n",
    "2. ** AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
